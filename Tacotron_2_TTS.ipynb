{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6obodDOS8cvK",
        "outputId": "04237c3b-0eb6-44eb-8a03-9b749ef6a607"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Farrell Arbiter\\TUGAS AKHIR 2\\Flask\\flask\\Tacotron_2_TTS.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39mdrive\u001b[39m\u001b[39m'\u001b[39m, force_remount\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbeFIdzp8oAq",
        "outputId": "e1f367e1-52fa-4841-b2bb-b1d588cac2ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 2] The system cannot find the file specified: '/content/'\n",
            "c:\\Farrell Arbiter\\TUGAS AKHIR 2\\Flask\\flask\n",
            "Cloning justinjohn0306/TTS-TT2\n",
            "[WinError 3] The system cannot find the path specified: '/content/TTS-TT2/'\n",
            "c:\\Farrell Arbiter\\TUGAS AKHIR 2\\Flask\\flask\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'TTS-TT2' already exists and is not an empty directory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 3] The system cannot find the path specified: '/content/TTS-TT2/'\n",
            "c:\\Farrell Arbiter\\TUGAS AKHIR 2\\Flask\\flask\n",
            "Downloading tacotron2 requirements\n",
            "Requirement already satisfied: matplotlib in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.7.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.1)\n",
            "Collecting inflect\n",
            "  Obtaining dependency information for inflect from https://files.pythonhosted.org/packages/fb/c6/d9feb758be584f729424390af24687d3a4363d968164f94079f83cd536b4/inflect-7.0.0-py3-none-any.whl.metadata\n",
            "  Downloading inflect-7.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: scipy in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.11.2)\n",
            "Requirement already satisfied: Unidecode in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.7)\n",
            "Requirement already satisfied: pillow in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (10.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nitro 5\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.8.2)\n",
            "Collecting pydantic>=1.9.1 (from inflect)\n",
            "  Obtaining dependency information for pydantic>=1.9.1 from https://files.pythonhosted.org/packages/73/66/0a72c9fcde42e5650c8d8d5c5c1873b9a3893018020c77ca8eb62708b923/pydantic-2.4.2-py3-none-any.whl.metadata\n",
            "  Downloading pydantic-2.4.2-py3-none-any.whl.metadata (158 kB)\n",
            "     ---------------------------------------- 0.0/158.6 kB ? eta -:--:--\n",
            "     --------- --------------------------- 41.0/158.6 kB 991.0 kB/s eta 0:00:01\n",
            "     -------------------------- ----------- 112.6/158.6 kB 1.3 MB/s eta 0:00:01\n",
            "     -------------------------------------- 158.6/158.6 kB 1.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from inflect) (4.8.0)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=1.9.1->inflect)\n",
            "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic-core==2.10.1 (from pydantic>=1.9.1->inflect)\n",
            "  Obtaining dependency information for pydantic-core==2.10.1 from https://files.pythonhosted.org/packages/7c/49/bd863a3d7c5412d739f3a28da0f437f25ccfa92413675c93412e64b812ed/pydantic_core-2.10.1-cp311-none-win_amd64.whl.metadata\n",
            "  Using cached pydantic_core-2.10.1-cp311-none-win_amd64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading inflect-7.0.0-py3-none-any.whl (34 kB)\n",
            "Downloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
            "   ---------------------------------------- 0.0/395.8 kB ? eta -:--:--\n",
            "   ------------ --------------------------- 122.9/395.8 kB 3.6 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 204.8/395.8 kB 2.1 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 337.9/395.8 kB 2.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 395.8/395.8 kB 2.5 MB/s eta 0:00:00\n",
            "Using cached pydantic_core-2.10.1-cp311-none-win_amd64.whl (2.0 MB)\n",
            "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: pydantic-core, annotated-types, pydantic, inflect\n",
            "Successfully installed annotated-types-0.6.0 inflect-7.0.0 pydantic-2.4.2 pydantic-core-2.10.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/wkentaro/gdown.git 'C:\\Users\\NITRO 5\\AppData\\Local\\Temp\\pip-req-build-c21mi_m2'\n",
            "  Running command git submodule update --init --recursive -q\n",
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/wkentaro/gdown.git\n",
            "  Cloning https://github.com/wkentaro/gdown.git to c:\\users\\nitro 5\\appdata\\local\\temp\\pip-req-build-c21mi_m2\n",
            "  Resolved https://github.com/wkentaro/gdown.git to commit 5c7507f02718048899b85d4010a6ed93316cbf27\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: filelock in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown==4.7.1) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown==4.7.1) (2.31.0)\n",
            "Requirement already satisfied: six in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown==4.7.1) (1.16.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown==4.7.1) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown==4.7.1) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->gdown==4.7.1) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown==4.7.1) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown==4.7.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown==4.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown==4.7.1) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown==4.7.1) (1.7.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\nitro 5\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->gdown==4.7.1) (0.4.6)\n",
            "Collecting ffmpeg-normalize\n",
            "  Obtaining dependency information for ffmpeg-normalize from https://files.pythonhosted.org/packages/07/d6/34e4582b2fda61335e73e8f28efb35eb9ae2a3a3456654cce97a004e8795/ffmpeg_normalize-1.27.7-py2.py3-none-any.whl.metadata\n",
            "  Downloading ffmpeg_normalize-1.27.7-py2.py3-none-any.whl.metadata (50 kB)\n",
            "     ---------------------------------------- 0.0/50.7 kB ? eta -:--:--\n",
            "     ------------------------ --------------- 30.7/50.7 kB 1.4 MB/s eta 0:00:01\n",
            "     -------------------------------------- 50.7/50.7 kB 860.7 kB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ffmpeg-normalize) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\nitro 5\\appdata\\roaming\\python\\python311\\site-packages (from ffmpeg-normalize) (0.4.6)\n",
            "Collecting ffmpeg-progress-yield (from ffmpeg-normalize)\n",
            "  Obtaining dependency information for ffmpeg-progress-yield from https://files.pythonhosted.org/packages/ea/b6/ba9d4b4ca272222f2858c8d2fac6ca9d74fb6494ea0449a3a07dafc07c56/ffmpeg_progress_yield-0.7.8-py2.py3-none-any.whl.metadata\n",
            "  Downloading ffmpeg_progress_yield-0.7.8-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting colorlog (from ffmpeg-normalize)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading ffmpeg_normalize-1.27.7-py2.py3-none-any.whl (38 kB)\n",
            "Downloading ffmpeg_progress_yield-0.7.8-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: ffmpeg-progress-yield, colorlog, ffmpeg-normalize\n",
            "Successfully installed colorlog-6.7.0 ffmpeg-normalize-1.27.7 ffmpeg-progress-yield-0.7.8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'chmod' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading tt2 pretrained\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: '/content/TTS-TT2'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Farrell Arbiter\\TUGAS AKHIR 2\\Flask\\flask\\Tacotron_2_TTS.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(\u001b[39m\"\u001b[39m\u001b[39m/content/TTS-TT2/pretrained_model\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDownloading tt2 pretrained\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m   gdown\u001b[39m.\u001b[39;49mdownload(tt2_pretrained, \u001b[39m\"\u001b[39;49m\u001b[39m/content/TTS-TT2/pretrained_model\u001b[39;49m\u001b[39m\"\u001b[39;49m, quiet\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m latest_downloaded \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gdown\\download.py:259\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39mif\u001b[39;00m output_is_path:\n\u001b[0;32m    258\u001b[0m     existing_tmp_files \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 259\u001b[0m     \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(osp\u001b[39m.\u001b[39;49mdirname(output) \u001b[39mor\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[0;32m    260\u001b[0m         \u001b[39mif\u001b[39;00m file\u001b[39m.\u001b[39mstartswith(osp\u001b[39m.\u001b[39mbasename(output)):\n\u001b[0;32m    261\u001b[0m             existing_tmp_files\u001b[39m.\u001b[39mappend(osp\u001b[39m.\u001b[39mjoin(osp\u001b[39m.\u001b[39mdirname(output), file))\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/TTS-TT2'"
          ]
        }
      ],
      "source": [
        "\n",
        "%matplotlib inline\n",
        "import os\n",
        "import io\n",
        "%cd /content/\n",
        "if not os.path.isdir(\"/content/TTS-TT2/\"):\n",
        "  print(\"Cloning justinjohn0306/TTS-TT2\")\n",
        "  !git clone https://github.com/justinjohn0306/ARPAtaco2.git TTS-TT2\n",
        "  %cd /content/TTS-TT2/\n",
        "  !git submodule init\n",
        "  !git submodule update\n",
        "%cd /content/TTS-TT2/\n",
        "#NVIDIA's requirements\n",
        "#I believe Colab gives us PyTorch and TF by default so we don't need anything else\n",
        "#Versions specified in requirements.txt have conflicts so that's why we simply get current versions\n",
        "print(\"Downloading tacotron2 requirements\")\n",
        "!pip install matplotlib numpy inflect scipy Unidecode pillow\n",
        "#Our requirements\n",
        "#We'll need gdown to download some really cool things\n",
        "!pip install git+https://github.com/wkentaro/gdown.git\n",
        "import gdown\n",
        "!git submodule init\n",
        "!git submodule update\n",
        "!pip install ffmpeg-normalize\n",
        "!pip install -q unidecode tensorboardX\n",
        "!apt-get -qq install sox\n",
        "!apt-get install pv\n",
        "!apt-get install jq\n",
        "!wget https://raw.githubusercontent.com/tonikelope/megadown/master/megadown -O megadown.sh\n",
        "!chmod 755 megadown.sh\n",
        "#Download NVIDIA's LJSpeech model\n",
        "tt2_pretrained = \"https://drive.google.com/uc?id=1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA\"\n",
        "if not os.path.isfile(\"/content/TTS-TT2/pretrained_model\"):\n",
        "  print(\"Downloading tt2 pretrained\")\n",
        "  gdown.download(tt2_pretrained, \"/content/TTS-TT2/pretrained_model\", quiet=False)\n",
        "\n",
        "latest_downloaded = None\n",
        "\n",
        "import time\n",
        "import logging\n",
        "\n",
        "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
        "logging.getLogger('numba').setLevel(logging.WARNING)\n",
        "logging.getLogger('librosa').setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "from numpy import finfo\n",
        "\n",
        "import torch\n",
        "from distributed import apply_gradient_allreduce\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from model import Tacotron2\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from loss_function import Tacotron2Loss\n",
        "from logger import Tacotron2Logger\n",
        "from hparams import create_hparams\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import layers\n",
        "from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "from text import text_to_sequence\n",
        "from math import e\n",
        "#from tqdm import tqdm # Terminal\n",
        "#from tqdm import tqdm_notebook as tqdm # Legacy Notebook TQDM\n",
        "from tqdm.notebook import tqdm # Modern Notebook TQDM\n",
        "from distutils.dir_util import copy_tree\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def download_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
        "\n",
        "def create_mels():\n",
        "    print(\"Generating Mels\")\n",
        "    stft = layers.TacotronSTFT(\n",
        "                hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "                hparams.mel_fmax)\n",
        "    def save_mel(filename):\n",
        "        audio, sampling_rate = load_wav_to_torch(filename)\n",
        "        if sampling_rate != stft.sampling_rate:\n",
        "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(filename,\n",
        "                sampling_rate, stft.sampling_rate))\n",
        "        audio_norm = audio / hparams.max_wav_value\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "        melspec = stft.mel_spectrogram(audio_norm)\n",
        "        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n",
        "        np.save(filename.replace('.wav', ''), melspec)\n",
        "\n",
        "    import glob\n",
        "    wavs = glob.glob('wavs/*.wav')\n",
        "    for i in tqdm(wavs):\n",
        "        save_mel(i)\n",
        "\n",
        "\n",
        "def reduce_tensor(tensor, n_gpus):\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
        "    rt /= n_gpus\n",
        "    return rt\n",
        "\n",
        "\n",
        "def init_distributed(hparams, n_gpus, rank, group_name):\n",
        "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
        "    print(\"Initializing Distributed\")\n",
        "\n",
        "    # Set cuda device so everything is done on the right GPU.\n",
        "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
        "\n",
        "    # Initialize distributed communication\n",
        "    dist.init_process_group(\n",
        "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
        "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
        "\n",
        "    print(\"Done initializing distributed\")\n",
        "\n",
        "\n",
        "def prepare_dataloaders(hparams):\n",
        "    # Get data, data loaders and collate function ready\n",
        "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
        "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
        "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        train_sampler = DistributedSampler(trainset)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        train_sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=hparams.batch_size, pin_memory=False,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "    return train_loader, valset, collate_fn\n",
        "\n",
        "\n",
        "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
        "    if rank == 0:\n",
        "        if not os.path.isdir(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "            os.chmod(output_directory, 0o775)\n",
        "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
        "    else:\n",
        "        logger = None\n",
        "    return logger\n",
        "\n",
        "\n",
        "def load_model(hparams):\n",
        "    model = Tacotron2(hparams).cuda()\n",
        "    if hparams.fp16_run:\n",
        "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model_dict = checkpoint_dict['state_dict']\n",
        "    if len(ignore_layers) > 0:\n",
        "        model_dict = {k: v for k, v in model_dict.items()\n",
        "                      if k not in ignore_layers}\n",
        "        dummy_dict = model.state_dict()\n",
        "        dummy_dict.update(model_dict)\n",
        "        model_dict = dummy_dict\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    learning_rate = checkpoint_dict['learning_rate']\n",
        "    iteration = checkpoint_dict['iteration']\n",
        "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
        "        checkpoint_path, iteration))\n",
        "    return model, optimizer, learning_rate, iteration\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
        "    import random\n",
        "    if True:\n",
        "        print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
        "            iteration, filepath))\n",
        "        try:\n",
        "            torch.save({'iteration': iteration,\n",
        "                    'state_dict': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'learning_rate': learning_rate}, filepath)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"interrupt received while saving, waiting for save to complete.\")\n",
        "            torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n",
        "        print(\"Model Saved\")\n",
        "\n",
        "def plot_alignment(alignment, info=None):\n",
        "    %matplotlib inline\n",
        "    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n",
        "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
        "                   interpolation='none')\n",
        "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    xlabel = 'Decoder timestep'\n",
        "    if info is not None:\n",
        "        xlabel += '\\n\\n' + info\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Encoder timestep')\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "    plt.show()\n",
        "\n",
        "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
        "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate):\n",
        "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
        "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
        "                                shuffle=False, batch_size=batch_size,\n",
        "                                pin_memory=False, collate_fn=collate_fn)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            if distributed_run:\n",
        "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_val_loss = loss.item()\n",
        "            val_loss += reduced_val_loss\n",
        "        val_loss = val_loss / (i + 1)\n",
        "\n",
        "    model.train()\n",
        "    if rank == 0:\n",
        "        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
        "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
        "        if hparams.show_alignments:\n",
        "            %matplotlib inline\n",
        "            _, mel_outputs, gate_outputs, alignments = y_pred\n",
        "            idx = random.randint(0, alignments.size(0) - 1)\n",
        "            plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
        "\n",
        "def train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n",
        "          rank, group_name, hparams, log_directory2, save_interval, backup_interval):\n",
        "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
        "\n",
        "    Params\n",
        "    ------\n",
        "    output_directory (string): directory to save checkpoints\n",
        "    log_directory (string) directory to save tensorboard logs\n",
        "    checkpoint_path(string): checkpoint path\n",
        "    n_gpus (int): number of gpus\n",
        "    rank (int): rank of current gpu\n",
        "    hparams (object): comma separated list of \"name=value\" pairs.\n",
        "    \"\"\"\n",
        "    if hparams.distributed_run:\n",
        "        init_distributed(hparams, n_gpus, rank, group_name)\n",
        "\n",
        "    torch.manual_seed(hparams.seed)\n",
        "    torch.cuda.manual_seed(hparams.seed)\n",
        "\n",
        "    model = load_model(hparams)\n",
        "    learning_rate = hparams.learning_rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=hparams.weight_decay)\n",
        "\n",
        "    if hparams.fp16_run:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(\n",
        "            model, optimizer, opt_level='O2')\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    criterion = Tacotron2Loss()\n",
        "\n",
        "    logger = prepare_directories_and_logger(\n",
        "        output_directory, log_directory, rank)\n",
        "\n",
        "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
        "\n",
        "    # Load checkpoint if one exists\n",
        "    iteration = 0\n",
        "    epoch_offset = 0\n",
        "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
        "        if warm_start:\n",
        "            model = warm_start_model(\n",
        "                checkpoint_path, model, hparams.ignore_layers)\n",
        "        else:\n",
        "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
        "                checkpoint_path, model, optimizer)\n",
        "            if hparams.use_saved_learning_rate:\n",
        "                learning_rate = _learning_rate\n",
        "            iteration += 1  # next iteration is iteration + 1\n",
        "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
        "    else:\n",
        "      os.path.isfile(\"/content/TTS-TT2/pretrained_model\")\n",
        "      %cd /dev/null\n",
        "      !/content/TTS-TT2/megadown.sh https://mega.nz/#!WXY3RILA!KyoGHtfB_sdhmLFoykG2lKWhh0GFdwMkk7OwAjpQHRo --o pretrained_model\n",
        "      %cd /content/TTS-TT2\n",
        "      model = warm_start_model(\"/content/TTS-TT2/pretrained_model\", model, hparams.ignore_layers)\n",
        "      # download LJSpeech pretrained model if no checkpoint already exists\n",
        "\n",
        "    start_eposh = time.perf_counter()\n",
        "    learning_rate = 0.0\n",
        "    model.train()\n",
        "    is_overflow = False\n",
        "    # ================ MAIN TRAINNIG LOOP! ===================\n",
        "    for epoch in tqdm(range(epoch_offset, hparams.epochs)):\n",
        "        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
        "        start_eposh = time.perf_counter()\n",
        "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "            start = time.perf_counter()\n",
        "            if iteration < hparams.decay_start: learning_rate = hparams.A_\n",
        "            else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted/hparams.B_))) + hparams.C_\n",
        "            learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = learning_rate\n",
        "\n",
        "            model.zero_grad()\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            if hparams.distributed_run:\n",
        "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_loss = loss.item()\n",
        "            if hparams.fp16_run:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            if hparams.fp16_run:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
        "                is_overflow = math.isnan(grad_norm)\n",
        "            else:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if not is_overflow and rank == 0:\n",
        "                duration = time.perf_counter() - start\n",
        "                logger.log_training(\n",
        "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
        "                #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
        "\n",
        "            iteration += 1\n",
        "        validate(model, criterion, valset, iteration,\n",
        "                 hparams.batch_size, n_gpus, collate_fn, logger,\n",
        "                 hparams.distributed_run, rank, epoch, start_eposh, learning_rate)\n",
        "        if (epoch+1) % save_interval == 0 or (epoch+1) == hparams.epochs: # not sure if the latter is necessary\n",
        "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
        "        if backup_interval > 0 and (epoch+1) % backup_interval == 0:\n",
        "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path + \"_epoch_%s\" % (epoch+1))\n",
        "        if log_directory2 != None:\n",
        "            copy_tree(log_directory, log_directory2)\n",
        "def check_dataset(hparams):\n",
        "    from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "    import os\n",
        "    import numpy as np\n",
        "    def check_arr(filelist_arr):\n",
        "        for i, file in enumerate(filelist_arr):\n",
        "            if len(file) > 2:\n",
        "                print(\"|\".join(file), \"\\nhas multiple '|', this may not be an error.\")\n",
        "            if hparams.load_mel_from_disk and '.wav' in file[0]:\n",
        "                print(\"[WARNING]\", file[0], \" in filelist while expecting .npy .\")\n",
        "            else:\n",
        "                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                    print(\"[WARNING]\", file[0], \" in filelist while expecting .wav .\")\n",
        "            if (not os.path.exists(file[0])):\n",
        "                print(\"|\".join(file), \"\\n[WARNING] does not exist.\")\n",
        "            if len(file[1]) < 3:\n",
        "                print(\"|\".join(file), \"\\n[info] has no/very little text.\")\n",
        "            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n",
        "                print(\"|\".join(file), \"\\n[info] has no ending punctuation.\")\n",
        "            mel_length = 1\n",
        "            if hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                melspec = torch.from_numpy(np.load(file[0], allow_pickle=True))\n",
        "                mel_length = melspec.shape[1]\n",
        "            if mel_length == 0:\n",
        "                print(\"|\".join(file), \"\\n[WARNING] has 0 duration.\")\n",
        "    print(\"Checking Training Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.training_files) # get split lines from training_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Checking Validation Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files) # get split lines from validation_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Finished Checking\")\n",
        "\n",
        "warm_start=False#sorry bout that\n",
        "n_gpus=1\n",
        "rank=0\n",
        "group_name=None\n",
        "\n",
        "# ---- DEFAULT PARAMETERS DEFINED HERE ----\n",
        "hparams = create_hparams()\n",
        "model_filename = 'current_model'\n",
        "hparams.training_files = \"filelists/clipper_train_filelist.txt\"\n",
        "hparams.validation_files = \"filelists/clipper_val_filelist.txt\"\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "hparams.decay_start = 15000\n",
        "hparams.A_ = 5e-4\n",
        "hparams.B_ = 8000\n",
        "hparams.C_ = 0\n",
        "hparams.min_learning_rate = 1e-5\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "hparams.batch_size = 32\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = []\n",
        "hparams.epochs = 10000\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "output_directory = '/content/drive/My Drive/colab/outdir' # Location to save Checkpoints\n",
        "log_directory = '/content/TTS-TT2/logs' # Location to save Log files locally\n",
        "log_directory2 = '/content/drive/My Drive/colab/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "!sed -i -- 's,.wav|,.npy|,g' filelists/*.txt\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "\n",
        "%cd /content/TTS-TT2\n",
        "\n",
        "data_path = 'wavs'\n",
        "!mkdir {data_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55yKT1wM9Ta9",
        "outputId": "4c946e70-35c6-4f8c-d2de-0a34f6bfe77e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/TTS-TT2/wavs\n",
            "\n",
            "\u001b[34m\u001b[1mAudio imported from Drive.\n",
            "\u001b[90m\n",
            "rm: cannot remove '/content/TTS-TT2/wavs/list.txt': No such file or directory\n",
            "\n",
            "\u001b[37mMetadata removal and audio verification...\n",
            "\n",
            "30 processed audios. total duration: 0:02:22\n",
            "\n",
            "\n",
            "\u001b[32m\u001b[1mAll set, please proceed.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "drive_path = \"/content/drive/MyDrive/wavs.zip\"\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "import wave\n",
        "import shutil\n",
        "import datetime\n",
        "\n",
        "if os.listdir('/content/TTS-TT2/wavs/'):\n",
        "  !rm /content/TTS-TT2/wavs/*\n",
        "\n",
        "with open('/content/audios.sh', 'w') as rsh:\n",
        "    rsh.write('''\\\n",
        "for file in /content/TTS-TT2/wavs/*.wav\n",
        "do\n",
        "    ffmpeg -y -i \"$file\" -ar 22050 /content/tempwav/srtmp.wav -loglevel error\n",
        "    ffmpeg -y -i /content/tempwav/srtmp.wav -c copy -fflags +bitexact -flags:v +bitexact -flags:a +bitexact -ar 22050 /content/tempwav/poop.wav -loglevel error\n",
        "    rm \"$file\"\n",
        "    mv /content/tempwav/poop.wav \"$file\"\n",
        "    rm /content/tempwav/*\n",
        "done\n",
        "''')\n",
        "\n",
        "%cd /content/TTS-TT2/wavs\n",
        "\n",
        "drive_path = drive_path.strip()\n",
        "\n",
        "if drive_path:\n",
        "  if os.path.exists(drive_path):\n",
        "    print(f\"\\n\\033[34m\\033[1mAudio imported from Drive.\\n\\033[90m\")\n",
        "    if zipfile.is_zipfile(drive_path):\n",
        "      !unzip -q -j \"$drive_path\" -d /content/TTS-TT2/wavs\n",
        "\n",
        "    else:\n",
        "      fp = drive_path + \"/.\"\n",
        "      !cp -a \"$fp\" \"/content/TTS-TT2/wavs\"\n",
        "  else:\n",
        "    print(f\"\\n\\033[33m\\033[1m[NOTICE] The path {drive_path} is not found, check for errors and try again.\")\n",
        "    print(f\"\\n\\033[34m\\033[1mUpload your dataset(audios)...\")\n",
        "    uploaded = files.upload()\n",
        "else:\n",
        "  print(f\"\\n\\033[34m\\033[1mUpload your dataset(audios)...\")\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  for fn in uploaded.keys():\n",
        "    if zipfile.is_zipfile(fn):\n",
        "      !unzip -q -j \"$fn\" -d /content/TTS-TT2/wavs\n",
        "      !rm \"$fn\"\n",
        "\n",
        "if os.path.exists(\"/content/TTS-TT2/wavs/wavs\"):\n",
        "    for file in os.listdir(\"/content/TTS-TT2/wavs/wavs\"):\n",
        "      !mv /content/TTS-TT2/wavs/wavs/\"$file\"  /content/TTS-TT2/wavs/\"$file\"\n",
        "!rm /content/TTS-TT2/wavs/list.txt\n",
        "audio_processing = True\n",
        "if audio_processing:\n",
        "  print(f\"\\n\\033[37mMetadata removal and audio verification...\")\n",
        "  !mkdir /content/tempwav\n",
        "  !bash /content/audios.sh\n",
        "\n",
        "totalduration = 0\n",
        "for file_name in [x for x in os.listdir() if os.path.isfile(x)]:\n",
        "    with wave.open(file_name, \"rb\") as wave_file:\n",
        "        frames = wave_file.getnframes()\n",
        "        rate = wave_file.getframerate()\n",
        "        duration = frames / float(rate)\n",
        "        totalduration += duration\n",
        "\n",
        "        if duration >= 12:\n",
        "          print(f\"\\n\\033[33m\\033[1m[NOTICE] {file_name} is longer than 12 seconds. Lack of RAM can\"\n",
        "                \" occur in a large batch size!\")\n",
        "\n",
        "wav_count = len(os.listdir(\"/content/TTS-TT2/wavs\"))\n",
        "print(f\"\\n{wav_count} processed audios. total duration: {str(datetime.timedelta(seconds=round(totalduration, 0)))}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\033[32m\\033[1mAll set, please proceed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "UhrGqTJS-Imr",
        "outputId": "e841f281-3342-4a39-f2ee-22e17d74caaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/TTS-TT2/filelists\n",
            "rm: cannot remove '/content/TTS-TT2/filelists/list.txt': No such file or directory\n",
            "\n",
            "\u001b[34m\u001b[1mUpload your transcript(list)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5bee6115-7033-48d4-87d8-2bb942d7a197\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5bee6115-7033-48d4-87d8-2bb942d7a197\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving list.txt to list.txt\n",
            "/content/TTS-TT2\n",
            "\n",
            "\u001b[32m\u001b[1mAll set, please proceed.\n"
          ]
        }
      ],
      "source": [
        "#upload transcript\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "%cd /content/TTS-TT2/filelists/\n",
        "!rm /content/TTS-TT2/filelists/list.txt\n",
        "\n",
        "print(\"\\n\\033[34m\\033[1mUpload your transcript(list)...\")\n",
        "listfn, length = files.upload().popitem()\n",
        "\n",
        "if listfn != \"list.txt\":\n",
        "  !mv \"$listfn\" list.txt\n",
        "\n",
        "\n",
        "with open('list.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "\n",
        "new_lines = []\n",
        "for line in lines:\n",
        "    audio_file_path = '/content/TTS-TT2/' + line.split('|')[0]\n",
        "\n",
        "    if os.path.exists(audio_file_path):\n",
        "        new_lines.append(line)\n",
        "\n",
        "\n",
        "with open('list.txt', 'w') as f:\n",
        "    f.writelines(new_lines)\n",
        "\n",
        "%cd /content/TTS-TT2/\n",
        "print(\"\\n\\033[32m\\033[1mAll set, please proceed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXohgloj-jU0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model_filename = 'TTS tacotron'\n",
        "\n",
        "\n",
        "Training_file = \"/content/TTS-TT2/filelists/list.txt\"\n",
        "hparams.training_files = Training_file\n",
        "hparams.validation_files = Training_file\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "\n",
        "\n",
        "hparams.decay_start = 15000\n",
        "\n",
        "hparams.A_ = 3e-4\n",
        "hparams.B_ = 8000                   # Decay Rate\n",
        "hparams.C_ = 0                      # Shift learning rate equation by this value\n",
        "hparams.min_learning_rate = 1e-5    # Min Learning Rate\n",
        "\n",
        "# Quality of Life\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "\n",
        "\n",
        "\n",
        "hparams.batch_size =  30\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = []\n",
        "use_cmudict = True\n",
        "\n",
        "hparams.epochs =  5\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "\n",
        "\n",
        "output_directory = '/content/drive/MyDrive/colab/outdir'\n",
        "log_directory = '/content/TTS-TT2/logs'\n",
        "log_directory2 = '/content/drive/My Drive/colab/logs'\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "hparams.text_cleaners=[\"english_cleaners\"] + ([\"cmudict_cleaners\"] if use_cmudict is True else [])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "369c4f45a01c43038b160d1c1207bdf3",
            "f181b5ed24574612ad890819a07dda7e",
            "1ff28a1b0c8c4e8db25c792865c6ae2e",
            "e5499f3009cc41409aa0c06f3f24975c",
            "c3bae91f22f04afba4fd563b7255ed9a",
            "b2bc47cd04a4465f894871c0a1a14df8",
            "af77411ee7c04b68910192b0e63c6cf7",
            "b5f6205d4794485f8fc83e25d98da056",
            "5124d006ce294a33b8f0c7092d295517",
            "f7753386bafb414da992b36475ac8b5d",
            "dc151a8f4a4049cb80461e0db29d4fae"
          ]
        },
        "id": "Qu6335yh_ew0",
        "outputId": "890fcaf8-7164-4d5a-cd32-b82a0accd087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating Mels\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "369c4f45a01c43038b160d1c1207bdf3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking for missing files\n",
            "Checking Training Files\n",
            "Checking Validation Files\n",
            "Finished Checking\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "if generate_mels:\n",
        "    create_mels()\n",
        "\n",
        "print(\"Checking for missing files\")\n",
        "#Replace .wav with .npy in filelists\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}; sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
        "\n",
        "check_dataset(hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuCHUJIt_q4P",
        "outputId": "f71334ec-fd3a-4a9a-b211-4883b8936770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/TTS-TT2\n",
            "{W IY1} {M AH1 S T} {K AE1 P CH ER0} {AE1 N} {ER1 TH} {K R IY1 CH ER0} , {K EY1} nine , {AH0 N D} {R IH0 T ER1 N} {IH1 T} {B AE1 K} {W IH1 DH} {AH1 S} {T UW1} {M AA1 R Z} .\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content/TTS-TT2/\n",
        "import text\n",
        "print(text.sequence_to_text(text.text_to_sequence(\"We must capture an Earth creature, K 9, and return it back with us to Mars.\", [\"cmudict_cleaners\", \"english_cleaners\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "9f4fbc4532634a5c89d214a8eb47e401",
            "78f95013ff2a411c916ebdf16034ffe8",
            "a371de23a5154731bd57db2a74924509",
            "40c84988399548b5935c3b390445ef1f",
            "20522cd3403640cea8f7982b72e1c3a2",
            "0b17352a787249818dc870e8b2a75072",
            "d2b5a201b32b46ec93cbef64256e093d",
            "fb598d27f35e459982855d70999beb10",
            "6b9b8daba4ed4b35baea1d79543671f5",
            "f9daa424b89747869f7eb50823d1fc12",
            "defa03a8cb2c473f97d58970005dc70d"
          ]
        },
        "id": "mTTsbtZU_1R0",
        "outputId": "d852a86c-08f4-47c0-b9a4-8777df5fbc7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FP16 Run: False\n",
            "Dynamic Loss Scaling: True\n",
            "Distributed Run: False\n",
            "cuDNN Enabled: True\n",
            "cuDNN Benchmark: False\n",
            "Loading checkpoint '/content/drive/MyDrive/colab/outdir/TTS tacotron'\n",
            "Loaded checkpoint '/content/drive/MyDrive/colab/outdir/TTS tacotron' from iteration 250\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f4fbc4532634a5c89d214a8eb47e401",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "save_interval =  10\n",
        "\n",
        "backup_interval =  -1\n",
        "\n",
        "print('FP16 Run:', hparams.fp16_run)\n",
        "print('Dynamic Loss Scaling:', hparams.dynamic_loss_scaling)\n",
        "print('Distributed Run:', hparams.distributed_run)\n",
        "print('cuDNN Enabled:', hparams.cudnn_enabled)\n",
        "print('cuDNN Benchmark:', hparams.cudnn_benchmark)\n",
        "train(output_directory, log_directory, checkpoint_path,\n",
        "      warm_start, n_gpus, rank, group_name, hparams, log_directory2,\n",
        "      save_interval, backup_interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 373,
          "referenced_widgets": [
            "aea3eb9f04f44cf9a81ea7f35d0cbbad",
            "4f354ce5e1a34f549bdb54101c6a7039",
            "9f925cee84034baa87edeced2834c009",
            "608494d1e1f147a2ae2cbd3a388169e1",
            "1ac37fa40f584de98ddf2b9467caf383",
            "95e7bfae632047f893181733bc27e71f",
            "c307625b9ee14443a1975eef6ebfee55",
            "a520ebc43da546068e45a7e9324b38b0",
            "9e6ca375d7c8457193d8f578326a1790",
            "5930676e8f5c4a25afb4fe2dcd297806",
            "22e3442e3ec14b418fa20c6035edfef8"
          ]
        },
        "id": "_L4iVzKDBwR6",
        "outputId": "61ccf531-31b0-40c9-e589-eeddc9b3270c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using universal Hifi-Gan model.\n",
            "Setting up, please wait.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be1d6c53082841eeb85ca33eeaa65592",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: resampy in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from resampy) (1.25.2)\n",
            "Requirement already satisfied: numba>=0.53 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from resampy) (0.58.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from numba>=0.53->resampy) (0.41.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/wkentaro/gdown.git 'C:\\Users\\NITRO 5\\AppData\\Local\\Temp\\pip-req-build-e3fr9i2m'\n",
            "  Running command git submodule update --init --recursive -q\n",
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/wkentaro/gdown.git\n",
            "  Cloning https://github.com/wkentaro/gdown.git to c:\\users\\nitro 5\\appdata\\local\\temp\\pip-req-build-e3fr9i2m\n",
            "  Resolved https://github.com/wkentaro/gdown.git to commit 5c7507f02718048899b85d4010a6ed93316cbf27\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: filelock in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown==4.7.1) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown==4.7.1) (2.31.0)\n",
            "Requirement already satisfied: six in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown==4.7.1) (1.16.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown==4.7.1) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gdown==4.7.1) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->gdown==4.7.1) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown==4.7.1) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown==4.7.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown==4.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown==4.7.1) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\nitro 5\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->gdown==4.7.1) (1.7.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\nitro 5\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->gdown==4.7.1) (0.4.6)\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "DLL load failed while importing lapack_lite: The specified module could not be found.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32mc:\\Farrell Arbiter\\TUGAS AKHIR 2\\Flask\\flask\\Tacotron_2_TTS.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#X13sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#X13sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhparams\u001b[39;00m \u001b[39mimport\u001b[39;00m create_hparams\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#X13sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m Tacotron2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#X13sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m TacotronSTFT\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Farrell%20Arbiter/TUGAS%20AKHIR%202/Flask/flask/Tacotron_2_TTS.ipynb#X13sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudio_processing\u001b[39;00m \u001b[39mimport\u001b[39;00m griffin_lim\n",
            "File \u001b[1;32mc:\\Farrell Arbiter\\TUGAS AKHIR 2\\Flask\\flask\\model.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvNorm, LinearNorm\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m to_gpu, get_mask_from_lengths\n\u001b[0;32m     10\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLocationLayer\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
            "File \u001b[1;32mc:\\Farrell Arbiter\\TUGAS AKHIR 2\\Flask\\flask\\layers.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlibrosa\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfilters\u001b[39;00m \u001b[39mimport\u001b[39;00m mel \u001b[39mas\u001b[39;00m librosa_mel_fn\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudio_processing\u001b[39;00m \u001b[39mimport\u001b[39;00m dynamic_range_compression\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39maudio_processing\u001b[39;00m \u001b[39mimport\u001b[39;00m dynamic_range_decompression\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\filters.py:49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msignal\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mndimage\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumba\u001b[39;00m \u001b[39mimport\u001b[39;00m jit\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\signal\\__init__.py:324\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_upfirdn\u001b[39;00m \u001b[39mimport\u001b[39;00m upfirdn\n\u001b[0;32m    316\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_spline\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     cspline2d,\n\u001b[0;32m    318\u001b[0m     qspline2d,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m     symiirorder2,\n\u001b[0;32m    322\u001b[0m )\n\u001b[1;32m--> 324\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bsplines\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_filter_design\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    326\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_fir_filter_design\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\signal\\_bsplines.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m comb\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_lib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_util\u001b[39;00m \u001b[39mimport\u001b[39;00m float_factorial\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpolate\u001b[39;00m \u001b[39mimport\u001b[39;00m BSpline\n\u001b[0;32m     14\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mspline_filter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbspline\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgauss_spline\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcubic\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mquadratic\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mcspline1d\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mqspline1d\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcspline1d_eval\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mqspline1d_eval\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspline_filter\u001b[39m(Iin, lmbda\u001b[39m=\u001b[39m\u001b[39m5.0\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\interpolate\\__init__.py:167\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m========================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mInterpolation (:mod:`scipy.interpolate`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39m(should not be used in new code).\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_interpolate\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    168\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_fitpack_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    170\u001b[0m \u001b[39m# New interface to fitpack library:\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\interpolate\\_interpolate.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspec\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m comb\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _fitpack_py\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dfitpack\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_polyint\u001b[39;00m \u001b[39mimport\u001b[39;00m _Interpolator1D\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\interpolate\\_fitpack_py.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_fitpack_impl\u001b[39;00m \u001b[39mimport\u001b[39;00m bisplrep, bisplev, dblint  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _fitpack_impl \u001b[39mas\u001b[39;00m _impl\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bsplines\u001b[39;00m \u001b[39mimport\u001b[39;00m BSpline\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplprep\u001b[39m(x, w\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, u\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ub\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ue\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, task\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, s\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, t\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m             full_output\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, nest\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, per\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, quiet\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     15\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m    Find the B-spline representation of an N-D curve.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \n\u001b[0;32m    153\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\interpolate\\_bsplines.py:9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmultiarray\u001b[39;00m \u001b[39mimport\u001b[39;00m normalize_axis_index\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m (get_lapack_funcs, LinAlgError,\n\u001b[0;32m      7\u001b[0m                           cholesky_banded, cho_solve_banded,\n\u001b[0;32m      8\u001b[0m                           solve, solve_banded)\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimize\u001b[39;00m \u001b[39mimport\u001b[39;00m minimize_scalar\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _bspl\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _fitpack_impl\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\__init__.py:410\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m=====================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mOptimization and root finding (:mod:`scipy.optimize`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    406\u001b[0m \n\u001b[0;32m    407\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optimize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m--> 410\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_minimize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    411\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_root\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    412\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_root_scalar\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_trustregion_krylov\u001b[39;00m \u001b[39mimport\u001b[39;00m _minimize_trust_krylov\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_trustregion_exact\u001b[39;00m \u001b[39mimport\u001b[39;00m _minimize_trustregion_exact\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_trustregion_constr\u001b[39;00m \u001b[39mimport\u001b[39;00m _minimize_trustregion_constr\n\u001b[0;32m     29\u001b[0m \u001b[39m# constrained minimization\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_lbfgsb_py\u001b[39;00m \u001b[39mimport\u001b[39;00m _minimize_lbfgsb\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_trustregion_constr\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"This module contains the equality constrained SQP solver.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mminimize_trustregion_constr\u001b[39;00m \u001b[39mimport\u001b[39;00m _minimize_trustregion_constr\n\u001b[0;32m      6\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m_minimize_trustregion_constr\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_trustregion_constr\\minimize_trustregion_constr.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearOperator\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_differentiable_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m VectorFunction\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_constraints\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     NonlinearConstraint, LinearConstraint, PreparedConstraint, strict_bounds)\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_hessian_update_strategy\u001b[39;00m \u001b[39mimport\u001b[39;00m BFGS\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_optimize\u001b[39;00m \u001b[39mimport\u001b[39;00m OptimizeResult\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\optimize\\_constraints.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optimize\u001b[39;00m \u001b[39mimport\u001b[39;00m OptimizeWarning\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarnings\u001b[39;00m \u001b[39mimport\u001b[39;00m warn, catch_warnings, simplefilter\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m suppress_warnings\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m issparse\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_arr_to_scalar\u001b[39m(x):\n\u001b[0;32m     13\u001b[0m     \u001b[39m# If x is a numpy array, return x.item().  This will\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[39m# fail if the array has more than one element.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\testing\\__init__.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munittest\u001b[39;00m \u001b[39mimport\u001b[39;00m TestCase\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _private\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m \u001b[39mimport\u001b[39;00m extbuild\n",
            "File \u001b[1;32mc:\\Users\\NITRO 5\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\testing\\_private\\utils.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m      intp, float32, empty, arange, array_repr, ndarray, isnat, array)\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m isfinite, isnan, isinf\n\u001b[1;32m---> 25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlapack_lite\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m StringIO\n\u001b[0;32m     29\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     30\u001b[0m         \u001b[39m'\u001b[39m\u001b[39massert_equal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_almost_equal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_approx_equal\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m         \u001b[39m'\u001b[39m\u001b[39massert_array_equal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_array_less\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_string_equal\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m_OLD_PROMOTION\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIS_MUSL\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_SUPPORTS_SVE\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     42\u001b[0m         ]\n",
            "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing lapack_lite: The specified module could not be found."
          ]
        }
      ],
      "source": [
        "#Run this code segment for using the trained model\n",
        "import logging\n",
        "\n",
        "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
        "logging.getLogger('numba').setLevel(logging.WARNING)\n",
        "logging.getLogger('librosa').setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "tacotron_id = \"1--Q11mrawG7-K5WUzJtDdQOYPXlA6zPk\"\n",
        "\n",
        "hifigan_id = \"universal\"\n",
        "\n",
        "\n",
        "\n",
        "if tacotron_id != \"\":\n",
        "    TACOTRON2_ID = tacotron_id\n",
        "else:\n",
        "    raise Exception(\"No ID provided.\")\n",
        "\n",
        "if hifigan_id in {\"\", \"universal\"}:\n",
        "    HIFIGAN_ID = \"universal\"\n",
        "    print(\"Using universal Hifi-Gan model.\")\n",
        "else:\n",
        "    HIFIGAN_ID = hifigan_id\n",
        "\n",
        "\n",
        "print(\"Setting up, please wait.\\n\")\n",
        "!pip install tqdm -q\n",
        "from tqdm.notebook import tqdm\n",
        "with tqdm(total=5, leave=False) as pbar:\n",
        "    import os\n",
        "    from os.path import exists, join, basename, splitext\n",
        "    !pip install resampy\n",
        "    !pip install git+https://github.com/wkentaro/gdown.git\n",
        "    git_repo_url = 'https://github.com/justinjohn0306/TTS-TT2.git'\n",
        "    project_name = splitext(basename(git_repo_url))[0]\n",
        "    if not exists(project_name):\n",
        "        # clone and install\n",
        "        !git clone -q --recursive {git_repo_url}\n",
        "        !git clone -q --recursive https://github.com/justinjohn0306/hifi-gan\n",
        "        !pip install -q unidecode\n",
        "    pbar.update(1) # downloaded TT2 and HiFi-GAN\n",
        "    import sys\n",
        "    sys.path.append('hifi-gan')\n",
        "    sys.path.append(project_name)\n",
        "    import time\n",
        "    import matplotlib\n",
        "    import matplotlib.pylab as plt\n",
        "    import gdown\n",
        "    d = 'https://drive.google.com/uc?id='\n",
        "\n",
        "    %matplotlib inline\n",
        "    import IPython.display as ipd\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import json\n",
        "    from hparams import create_hparams\n",
        "    from model import Tacotron2\n",
        "    from layers import TacotronSTFT\n",
        "    from audio_processing import griffin_lim\n",
        "    from text import text_to_sequence\n",
        "    from env import AttrDict\n",
        "    from meldataset import mel_spectrogram, MAX_WAV_VALUE\n",
        "    from models import Generator\n",
        "    from denoiser import Denoiser\n",
        "    import resampy\n",
        "    import scipy.signal\n",
        "\n",
        "    pbar.update(1) # initialized Dependancies\n",
        "\n",
        "    graph_width = 900\n",
        "    graph_height = 360\n",
        "    def plot_data(data, figsize=(int(graph_width/100), int(graph_height/100))):\n",
        "        %matplotlib inline\n",
        "        fig, axes = plt.subplots(1, len(data), figsize=figsize)\n",
        "        for i in range(len(data)):\n",
        "            axes[i].imshow(data[i], aspect='auto', origin='lower',\n",
        "                        interpolation='none', cmap='inferno')\n",
        "        fig.canvas.draw()\n",
        "        plt.show()\n",
        "\n",
        "    # Setup Pronounciation Dictionary\n",
        "    !wget 'https://github.com/justinjohn0306/FakeYou-Tacotron2-Notebook/releases/download/CMU_dict/merged.dict.txt'\n",
        "    thisdict = {}\n",
        "    for line in reversed((open('merged.dict.txt', \"r\").read()).splitlines()):\n",
        "        thisdict[(line.split(\" \",1))[0]] = (line.split(\" \",1))[1].strip()\n",
        "\n",
        "    pbar.update(1) # Downloaded and Set up Pronounciation Dictionary\n",
        "\n",
        "    def ARPA(text, punctuation=r\"!?,.;\", EOS_Token=True):\n",
        "        out = ''\n",
        "        for word_ in text.split(\" \"):\n",
        "            word=word_; end_chars = ''\n",
        "            while any(elem in word for elem in punctuation) and len(word) > 1:\n",
        "                if word[-1] in punctuation: end_chars = word[-1] + end_chars; word = word[:-1]\n",
        "                else: break\n",
        "            try:\n",
        "                word_arpa = thisdict[word.upper()]\n",
        "                word = \"{\" + str(word_arpa) + \"}\"\n",
        "            except KeyError: pass\n",
        "            out = (out + \" \" + word + end_chars).strip()\n",
        "        if EOS_Token and out[-1] != \";\": out += \";\"\n",
        "        return out\n",
        "\n",
        "    def get_hifigan(MODEL_ID, conf_name):\n",
        "        # Download HiFi-GAN\n",
        "        hifigan_pretrained_model = 'hifimodel_' + conf_name\n",
        "        #gdown.download(d+MODEL_ID, hifigan_pretrained_model, quiet=False)\n",
        "\n",
        "        if MODEL_ID == 1:\n",
        "            !wget \"https://github.com/justinjohn0306/tacotron2/releases/download/assets/Superres_Twilight_33000\" -O $hifigan_pretrained_model\n",
        "        elif MODEL_ID == \"universal\":\n",
        "            !wget \"https://github.com/justinjohn0306/tacotron2/releases/download/assets/g_02500000\" -O $hifigan_pretrained_model\n",
        "        else:\n",
        "            gdown.download(d+MODEL_ID, hifigan_pretrained_model, quiet=False)\n",
        "\n",
        "        # Load HiFi-GAN\n",
        "        conf = os.path.join(\"hifi-gan\", conf_name + \".json\")\n",
        "        with open(conf) as f:\n",
        "            json_config = json.loads(f.read())\n",
        "        h = AttrDict(json_config)\n",
        "        torch.manual_seed(h.seed)\n",
        "        hifigan = Generator(h).to(torch.device(\"cuda\"))\n",
        "        state_dict_g = torch.load(hifigan_pretrained_model, map_location=torch.device(\"cuda\"))\n",
        "        hifigan.load_state_dict(state_dict_g[\"generator\"])\n",
        "        hifigan.eval()\n",
        "        hifigan.remove_weight_norm()\n",
        "        denoiser = Denoiser(hifigan, mode=\"normal\")\n",
        "        return hifigan, h, denoiser\n",
        "\n",
        "    # Download character HiFi-GAN\n",
        "    hifigan, h, denoiser = get_hifigan(HIFIGAN_ID, \"config_v1\")\n",
        "    # Download super-resolution HiFi-GAN\n",
        "    hifigan_sr, h2, denoiser_sr = get_hifigan(1, \"config_32k\")\n",
        "    pbar.update(1) # Downloaded and Set up HiFi-GAN\n",
        "\n",
        "    def has_MMI(STATE_DICT):\n",
        "        return any(True for x in STATE_DICT.keys() if \"mi.\" in x)\n",
        "\n",
        "    def get_Tactron2(MODEL_ID):\n",
        "        # Download Tacotron2\n",
        "        tacotron2_pretrained_model = 'MLPTTS'\n",
        "        gdown.download(d+MODEL_ID, tacotron2_pretrained_model, quiet=False)\n",
        "        if not exists(tacotron2_pretrained_model):\n",
        "            raise Exception(\"Tacotron2 model failed to download!\")\n",
        "        # Load Tacotron2 and Config\n",
        "        hparams = create_hparams()\n",
        "        hparams.sampling_rate = 22050\n",
        "        hparams.max_decoder_steps = 3000 # Max Duration\n",
        "        hparams.gate_threshold = 0.25 # Model must be 25% sure the clip is over before ending generation\n",
        "        model = Tacotron2(hparams)\n",
        "        state_dict = torch.load(tacotron2_pretrained_model)['state_dict']\n",
        "        if has_MMI(state_dict):\n",
        "            raise Exception(\"ERROR: This notebook does not currently support MMI models.\")\n",
        "        model.load_state_dict(state_dict)\n",
        "        _ = model.cuda().eval().half()\n",
        "        return model, hparams\n",
        "\n",
        "    model, hparams = get_Tactron2(TACOTRON2_ID)\n",
        "    previous_tt2_id = TACOTRON2_ID\n",
        "\n",
        "    pbar.update(1) # Downloaded and Set up Tacotron2\n",
        "\n",
        "    # Extra Info\n",
        "    def end_to_end_infer(text, pronounciation_dictionary, show_graphs):\n",
        "        for i in [x for x in text.split(\"\\n\") if len(x)]:\n",
        "            if not pronounciation_dictionary:\n",
        "                if i[-1] != \";\": i=i+\";\"\n",
        "            else: i = ARPA(i)\n",
        "            with torch.no_grad(): # save VRAM by not including gradients\n",
        "                sequence = np.array(text_to_sequence(i, ['english_cleaners']))[None, :]\n",
        "                sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long()\n",
        "                mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
        "                if show_graphs:\n",
        "                    plot_data((mel_outputs_postnet.float().data.cpu().numpy()[0],\n",
        "                            alignments.float().data.cpu().numpy()[0].T))\n",
        "                y_g_hat = hifigan(mel_outputs_postnet.float())\n",
        "                audio = y_g_hat.squeeze()\n",
        "                audio = audio * MAX_WAV_VALUE\n",
        "                audio_denoised = denoiser(audio.view(1, -1), strength=35)[:, 0]\n",
        "\n",
        "                # Resample to 32k\n",
        "                audio_denoised = audio_denoised.cpu().numpy().reshape(-1)\n",
        "\n",
        "                normalize = (MAX_WAV_VALUE / np.max(np.abs(audio_denoised))) ** 0.9\n",
        "                audio_denoised = audio_denoised * normalize\n",
        "                wave = resampy.resample(\n",
        "                    audio_denoised,\n",
        "                    h.sampling_rate,\n",
        "                    h2.sampling_rate,\n",
        "                    filter=\"sinc_window\",\n",
        "                    window=scipy.signal.windows.hann,\n",
        "                    num_zeros=8,\n",
        "                )\n",
        "                wave_out = wave.astype(np.int16)\n",
        "\n",
        "                # HiFi-GAN super-resolution\n",
        "                wave = wave / MAX_WAV_VALUE\n",
        "                wave = torch.FloatTensor(wave).to(torch.device(\"cuda\"))\n",
        "                new_mel = mel_spectrogram(\n",
        "                    wave.unsqueeze(0),\n",
        "                    h2.n_fft,\n",
        "                    h2.num_mels,\n",
        "                    h2.sampling_rate,\n",
        "                    h2.hop_size,\n",
        "                    h2.win_size,\n",
        "                    h2.fmin,\n",
        "                    h2.fmax,\n",
        "                )\n",
        "                y_g_hat2 = hifigan_sr(new_mel)\n",
        "                audio2 = y_g_hat2.squeeze()\n",
        "                audio2 = audio2 * MAX_WAV_VALUE\n",
        "                audio2_denoised = denoiser(audio2.view(1, -1), strength=35)[:, 0]\n",
        "\n",
        "                # High-pass filter, mixing and denormalizing\n",
        "                audio2_denoised = audio2_denoised.cpu().numpy().reshape(-1)\n",
        "                b = scipy.signal.firwin(\n",
        "                    101, cutoff=10500, fs=h2.sampling_rate, pass_zero=False\n",
        "                )\n",
        "                y = scipy.signal.lfilter(b, [1.0], audio2_denoised)\n",
        "                y *= superres_strength\n",
        "                y_out = y.astype(np.int16)\n",
        "                y_padded = np.zeros(wave_out.shape)\n",
        "                y_padded[: y_out.shape[0]] = y_out\n",
        "                sr_mix = wave_out + y_padded\n",
        "                sr_mix = sr_mix / normalize\n",
        "\n",
        "                print(\"\")\n",
        "                ipd.display(ipd.Audio(sr_mix.astype(np.int16), rate=h2.sampling_rate))\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "initialized = \"Ready\"\n",
        "\n",
        "if previous_tt2_id != TACOTRON2_ID:\n",
        "    print(\"Updating Models\")\n",
        "    model, hparams = get_Tactron2(TACOTRON2_ID)\n",
        "    hifigan, h, denoiser = get_hifigan(HIFIGAN_ID, \"config_v1\")\n",
        "    previous_tt2_id = TACOTRON2_ID\n",
        "\n",
        "pronounciation_dictionary = False\n",
        "# disables automatic ARPAbet conversion, useful for inputting your own ARPAbet pronounciations or just for testing\n",
        "show_graphs = True\n",
        "max_duration = 20\n",
        "model.decoder.max_decoder_steps = max_duration * 80\n",
        "stop_threshold = 0.5\n",
        "model.decoder.gate_threshold = stop_threshold\n",
        "superres_strength = 10\n",
        "\n",
        "\n",
        "print(f\"Current Config:\\npronounciation_dictionary: {pronounciation_dictionary}\\nshow_graphs: {show_graphs}\\nmax_duration (in seconds): {max_duration}\\nstop_threshold: {stop_threshold}\\nsuperres_strength: {superres_strength}\\n\\n\")\n",
        "\n",
        "time.sleep(1)\n",
        "print(\"Enter/Paste your text.\")\n",
        "contents = []\n",
        "while True:\n",
        "    try:\n",
        "        print(\"-\"*50)\n",
        "        line = input()\n",
        "        if line == \"\":\n",
        "            continue\n",
        "        end_to_end_infer(line, not pronounciation_dictionary, show_graphs)\n",
        "    except EOFError:\n",
        "        break\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Stopping...\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FDh3qF8Kg6om",
        "outputId": "7aae96a2-3621-40b6-b8c9-e67ea7d01dbe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/hifi-gan.zip'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import shutil\n",
        "shutil.make_archive('hifi-gan', 'zip', 'hifi-gan')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b17352a787249818dc870e8b2a75072": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac37fa40f584de98ddf2b9467caf383": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ff28a1b0c8c4e8db25c792865c6ae2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5f6205d4794485f8fc83e25d98da056",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5124d006ce294a33b8f0c7092d295517",
            "value": 30
          }
        },
        "20522cd3403640cea8f7982b72e1c3a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e3442e3ec14b418fa20c6035edfef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "369c4f45a01c43038b160d1c1207bdf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f181b5ed24574612ad890819a07dda7e",
              "IPY_MODEL_1ff28a1b0c8c4e8db25c792865c6ae2e",
              "IPY_MODEL_e5499f3009cc41409aa0c06f3f24975c"
            ],
            "layout": "IPY_MODEL_c3bae91f22f04afba4fd563b7255ed9a"
          }
        },
        "40c84988399548b5935c3b390445ef1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9daa424b89747869f7eb50823d1fc12",
            "placeholder": "​",
            "style": "IPY_MODEL_defa03a8cb2c473f97d58970005dc70d",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "4f354ce5e1a34f549bdb54101c6a7039": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95e7bfae632047f893181733bc27e71f",
            "placeholder": "​",
            "style": "IPY_MODEL_c307625b9ee14443a1975eef6ebfee55",
            "value": "  0%"
          }
        },
        "5124d006ce294a33b8f0c7092d295517": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5930676e8f5c4a25afb4fe2dcd297806": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608494d1e1f147a2ae2cbd3a388169e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5930676e8f5c4a25afb4fe2dcd297806",
            "placeholder": "​",
            "style": "IPY_MODEL_22e3442e3ec14b418fa20c6035edfef8",
            "value": " 0/5 [00:00&lt;?, ?it/s]"
          }
        },
        "6b9b8daba4ed4b35baea1d79543671f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "78f95013ff2a411c916ebdf16034ffe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b17352a787249818dc870e8b2a75072",
            "placeholder": "​",
            "style": "IPY_MODEL_d2b5a201b32b46ec93cbef64256e093d",
            "value": ""
          }
        },
        "95e7bfae632047f893181733bc27e71f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6ca375d7c8457193d8f578326a1790": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f4fbc4532634a5c89d214a8eb47e401": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78f95013ff2a411c916ebdf16034ffe8",
              "IPY_MODEL_a371de23a5154731bd57db2a74924509",
              "IPY_MODEL_40c84988399548b5935c3b390445ef1f"
            ],
            "layout": "IPY_MODEL_20522cd3403640cea8f7982b72e1c3a2"
          }
        },
        "9f925cee84034baa87edeced2834c009": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a520ebc43da546068e45a7e9324b38b0",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e6ca375d7c8457193d8f578326a1790",
            "value": 0
          }
        },
        "a371de23a5154731bd57db2a74924509": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb598d27f35e459982855d70999beb10",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b9b8daba4ed4b35baea1d79543671f5",
            "value": 0
          }
        },
        "a520ebc43da546068e45a7e9324b38b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aea3eb9f04f44cf9a81ea7f35d0cbbad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f354ce5e1a34f549bdb54101c6a7039",
              "IPY_MODEL_9f925cee84034baa87edeced2834c009",
              "IPY_MODEL_608494d1e1f147a2ae2cbd3a388169e1"
            ],
            "layout": "IPY_MODEL_1ac37fa40f584de98ddf2b9467caf383"
          }
        },
        "af77411ee7c04b68910192b0e63c6cf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2bc47cd04a4465f894871c0a1a14df8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f6205d4794485f8fc83e25d98da056": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c307625b9ee14443a1975eef6ebfee55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3bae91f22f04afba4fd563b7255ed9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2b5a201b32b46ec93cbef64256e093d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc151a8f4a4049cb80461e0db29d4fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "defa03a8cb2c473f97d58970005dc70d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5499f3009cc41409aa0c06f3f24975c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7753386bafb414da992b36475ac8b5d",
            "placeholder": "​",
            "style": "IPY_MODEL_dc151a8f4a4049cb80461e0db29d4fae",
            "value": " 30/30 [00:00&lt;00:00, 45.80it/s]"
          }
        },
        "f181b5ed24574612ad890819a07dda7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2bc47cd04a4465f894871c0a1a14df8",
            "placeholder": "​",
            "style": "IPY_MODEL_af77411ee7c04b68910192b0e63c6cf7",
            "value": "100%"
          }
        },
        "f7753386bafb414da992b36475ac8b5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9daa424b89747869f7eb50823d1fc12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb598d27f35e459982855d70999beb10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
